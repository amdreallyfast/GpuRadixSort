/*------------------------------------------------------------------------------------------------
Description:
    This is a parallel prefix sums algorithm that uses shared memory, a binary tree, and no 
    atomic counters to build up a prefix sum within a work group.  This is advantageous because 
    it uses fast shared memory instead of having many threads get in line to use atomic 
    counters, of which there are a limited number, on global memory.

    Thanks to developer.nvidia.com, GPU Gems 3, Chapter 39. Parallel Prefix Sum (Scan) with CUDA
    for the algorithm (despite the code golfing variable names and lack of comments, at least 
    they had pictures that I could eventually work out).
    http://http.developer.nvidia.com/GPUGems3/gpugems3_ch39.html
Creator:    John Cox, 3/11/2017
------------------------------------------------------------------------------------------------*/

// REQUIRES Version.comp
// REQUIRES ParallelSortConstants.comp
// - PARALLEL_SORT_WORK_GROUP_SIZE_X
// - ITEMS_PER_WORK_GROUP
// REQUIRES UniformLocations.comp
// REQUIRES SsboBufferBindings.comp
// REQUIRES PrefixScanBuffer.comp.comp

// Y and Z work group sizes default to 1
layout (local_size_x = PARALLEL_SORT_WORK_GROUP_SIZE_X) in;

// create a shared memory buffer for fast memory operations (better than global), two items per 
// thread
// Note: By definition of keyword "shared", this is shared amongst all threads in a work group.
// Also Note: Every item gets written before it is read, so don't bother initializing the 0.
shared uint[ITEMS_PER_WORK_GROUP] fastTempArr;// = uint[ITEMS_PER_WORK_GROUP](0);

/*------------------------------------------------------------------------------------------------
Description:
    This is where the magic happens.  Each thread in a work group copies two items from global 
    memory to work-group-shared memory, waits for all the other threads to catch up, performs 
    the scan (up the tree and back down), then writes their two result back to global memory.
Parameters: None
Returns:    None
Creator:    John Cox, 3/16/2017
------------------------------------------------------------------------------------------------*/
void CalculatePrefixSumsWithinGroup()
{
    // wait for all the threads in this group to do the shared memory initialization
    // Note: Consider this code: 
    //  shared uint[ITEMS_PER_WORK_GROUP] fastTempArr = uint[ITEMS_PER_WORK_GROUP](0);
    // The left side of that statement is shared between all values of the work group, but after 
    // some frustration and experimentation, I found that the right side seems to be run per 
    // thread.  Not all the threads in a group will run simultaneously, so to prevent some 
    // initial assignment of data followed by another thread in the group initializing the array 
    // back to 0, make everyone wait until initialization is done.
    barrier();

    // doubled because each thread deals with 2 items, so the shared data size is double the 
    // work group size, and everything dealing with indices also doubles them, so just make a 
    // doubled variable up front
    uint doubleGroupThreadIndex = gl_LocalInvocationID.x * 2;
    uint doubleGlobalThreadIndex = gl_GlobalInvocationID.x * 2;

    // Copy from global to shared data for a faster algorithm (and easier index calculations)
    // Note: Two elements per thread.
    fastTempArr[doubleGroupThreadIndex] = PrefixSumsWithinGroup[doubleGlobalThreadIndex];
    fastTempArr[doubleGroupThreadIndex + 1] = PrefixSumsWithinGroup[doubleGlobalThreadIndex + 1];

    // called simply "offset" in the GPU Gems article, this is a multiplier that works in 
    // conjunction with the thread number to calculate which index pairs are being considered on 
    // each loop by each thread
    uint indexMultiplierDueToDepth = 1;

    // going up divides pair count in half with each level
    for (uint dataPairs = ITEMS_PER_WORK_GROUP >> 1; dataPairs > 0; dataPairs >>= 1)
    {
        // wait for other threads in the group to catch up
        barrier();

        // one pair per thread
        // Note: Going up the tree will require fewer pair operations at each level.  Local 
        // thread ID 0 will always be doing something, but higher thread numbers will start 
        // sitting out until the "going down the tree" loop
        if (gl_LocalInvocationID.x < dataPairs)
        {
            uint lesserIndex = (indexMultiplierDueToDepth * (doubleGroupThreadIndex + 1)) - 1;
            uint greaterIndex = (indexMultiplierDueToDepth * (doubleGroupThreadIndex + 2)) - 1;

            fastTempArr[greaterIndex] += fastTempArr[lesserIndex];
        }

        // this is used in the "going down" loop, so do this even if the thread didn't do 
        // anything on this iteration
        indexMultiplierDueToDepth *= 2;
    }

    // only one thread should do these (prevents unnecessary writes)
    if (doubleGroupThreadIndex == 0)
    {
        // write the work group sum to the group sums buffer
        // Note: After the "going up" loop finishes, the last item in the shared memory array 
        // has the sum of all items in the entire array.  The following "going down" loop will 
        // change the data into a prefix-only sums array, so record the entire sum while it is 
        // still available.
        PrefixSumsByGroup[gl_WorkGroupID.x] = fastTempArr[ITEMS_PER_WORK_GROUP - 1];
       
        // this is just part of the algorithm; I don't have an intuitive explanation
        fastTempArr[ITEMS_PER_WORK_GROUP - 1] = 0;
    }

    // undo the last loop's indexMultiplierDueToDepth
    // Note: After the last loop, indexMultiplierDueToDepth had been multiplied by 2 as many 
    // times as dataPairs had been divided by 2, so it is now equivalent to ITEMS_PER_WORK_GROUP 
    // (assuming that it is a power of 2).  Divide by 2 so that it can be used to calculate the 
    // indices of the first data pair off the root.
    indexMultiplierDueToDepth >>= 1;

    // going down multiplies pair count in half with each level
    for (uint dataPairs = 1; dataPairs < ITEMS_PER_WORK_GROUP; dataPairs *= 2)
    {
        // wait for the other threads in the group to catch up
        barrier();

        // once again, group thread 0 is always working, but the others may need to sit out for 
        // a few loops
        if (gl_LocalInvocationID.x < dataPairs)
        {
            uint lesserIndex = (indexMultiplierDueToDepth * (doubleGroupThreadIndex + 1)) - 1;
            uint greaterIndex = (indexMultiplierDueToDepth * (doubleGroupThreadIndex + 2)) - 1;

            // this is a swap and a sum, so need a temporary value
            uint temp = fastTempArr[lesserIndex];
            fastTempArr[lesserIndex] = fastTempArr[greaterIndex];
            fastTempArr[greaterIndex] += temp;
        }

        // next level down will have twice the number of data pairs, so each index calculation 
        // needs half the offset due to depth
        indexMultiplierDueToDepth >>= 1;
    }

    // write the data back, two elements per thread, but wait for all the group threads to 
    // finish their loops first
    barrier();
    PrefixSumsWithinGroup[doubleGlobalThreadIndex] = fastTempArr[doubleGroupThreadIndex];
    PrefixSumsWithinGroup[doubleGlobalThreadIndex + 1] = fastTempArr[doubleGroupThreadIndex + 1];
}

/*------------------------------------------------------------------------------------------------
Description:
    Like CalculatePrefixSumsWithinGroup(), but only for the PrefixSumsByGroup.
Parameters: None
Returns:    None
Creator:    John Cox, 3/16/2017
------------------------------------------------------------------------------------------------*/
void PrefixSumOnPrefixSumsByGroup()
{
    // the PrefixSumsByGroup array is only one work group's worth of data, so the starting 
    // index is implicitly 0 and there is no need for a workGroupStartingIndex
    uint doubleGroupThreadIndex = gl_LocalInvocationID.x * 2;

    uint indexMultiplierDueToDepth = 1;
    for (int dataPairs = ITEMS_PER_WORK_GROUP >> 1; dataPairs > 0; dataPairs >>= 1)
    {
        barrier();

        if (gl_LocalInvocationID.x < dataPairs)
        {
            uint lesserIndex = (indexMultiplierDueToDepth * (doubleGroupThreadIndex + 1)) - 1;
            uint greaterIndex = (indexMultiplierDueToDepth * (doubleGroupThreadIndex + 2)) - 1;

            PrefixSumsByGroup[greaterIndex] += PrefixSumsByGroup[lesserIndex];
        }
        indexMultiplierDueToDepth *= 2;
    }

    // only one thread should do these (prevents unnecessary writes)
    if (doubleGroupThreadIndex == 0)
    {
        totalNumberOfOnes = PrefixSumsByGroup[ITEMS_PER_WORK_GROUP - 1];
        PrefixSumsByGroup[ITEMS_PER_WORK_GROUP - 1] = 0;
    }
    indexMultiplierDueToDepth >>= 1;
    
    for (int dataPairs = 1; dataPairs < ITEMS_PER_WORK_GROUP; dataPairs *= 2)
    {
        barrier();
        
        if (gl_LocalInvocationID.x < dataPairs)
        {
            uint lesserIndex = (indexMultiplierDueToDepth * (doubleGroupThreadIndex + 1)) - 1;
            uint greaterIndex = (indexMultiplierDueToDepth * (doubleGroupThreadIndex + 2)) - 1;
    
            uint temp = PrefixSumsByGroup[lesserIndex];
            PrefixSumsByGroup[lesserIndex] = PrefixSumsByGroup[greaterIndex];
            PrefixSumsByGroup[greaterIndex] += temp;
        }
    
        indexMultiplierDueToDepth >>= 1;
    }
}


layout(location = UNIFORM_LOCATION_CALCULATE_ALL) uniform uint uCalculateAll;

/*------------------------------------------------------------------------------------------------
Description:
    Determine whether to run the parallel prefix scan algorithm over all the data entries or 
    over the per-work-group sums.
Parameters: None
Returns:    None
Creator:    John Cox, 3/11/2017
------------------------------------------------------------------------------------------------*/
void main()
{
    // this algorithm is closely tied to the size of a work group, and when setting up for it 
    // care is taken to make sure that all threads within a work group have something to do, so 
    // as long as care keeps being taken to make sure that all threads are busy, then there is 
    // no need for a "max thread count" check or something like that

    if (uCalculateAll == 1)
    {
        // calculate the prefix sums for everyone and fill out the per-work-group sums
        CalculatePrefixSumsWithinGroup();
    }
    else
    {
        // calculate the prefix sums for the collection of per-work-group sums, turning this 
        // array from a collection of individual sums into a prefix sum array itself
        PrefixSumOnPrefixSumsByGroup();
    }

    // done!
}

